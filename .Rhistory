6000/750
ncont = 10072
nexp = 9886
xcont = 974
xexp = 1242
pool = (xcont + xexp)/(ncont + nexp)
pool
psd = sqrt(pool*(1-pool)*(1/ncont + 1/nexp))
psd
pd = xexp/nexp - xcont/ncont
pd
1.96*psd
pd + 1.96*psd
pd - 1.96*psd
272*482
1180*82
8845.62 - 5323
(8845.62 - 5323 - 2500)
(8845.62 - 5323 - 2000)
70000/48*0.73
720/4
4048.5/6
1064+ 90 + 67+ 50 + 38+1630 + 59 + 52+675+300+300+231+100+100+97+50
1064+ 90 + 67+ 50 + 38+1630 + 59 + 52+675+300+300+231+100+100+97+50 +80
7000 - 4983
(7000 - 4983)*12
(7000 - 4983)*12 - 19000*0.7
3536.35*2
3536.35*2*12
73701 + 67459 - 31000
110160/5000
110160/(5000-1064)
20000* 7.5
950*12 + 9600
(950*12 + 9600)*0.3
(950*12 + 9600)*0.3 + 680*12
(7.5-0.73)*17500
(7.5-0.73)*17500+135000
157.199
157/199
157/199-1
249/271
10197/251
300/5
300/5/12
(10000 + 580)/12
(10000 + 580)/12+1624
install.packages(c("dygraphs", "leaflet", "maps", "pageviews", "xts"))
2020-1997
75*4*5
75*4*5*4
75*4*7*4
230*30/1024
library(twitteR)
library(ROAuth)
library(httr)
library(rtweet)
library(readr)
library(dplyr)
# Set API Keys
api_key <- "bnu4FLGSFTJTh8mGlkbrpzhez"
api_secret <- "8zb5QollQqdUYF0Cm3MHhXWQax575qfGWrQ2w5nBRAVrHN1ONo"
access_token <- "174467976-UjZGpFMIEIvXGVUG34SiHGvmYeDXOx1ZimmcZUE3"
access_token_secret <- "VCnGpyY7UHpFPLfy5eRpn0r6hLRbAQqSVx6LlPhezYgd8"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
tweets_vercel <- searchTwitter('vercel', n = 10000)%>%
twListToDF()
View(tweets_vercel)
tweets_vercel <- searchTwitter('vercel', since = '2020-05-01')%>%
twListToDF()
View(tweets_vercel)
tweets_vercel <- searchTwitter('vercel', since = '2020-05-15 16:31:41')%>%
twListToDF()
tweets_vercel <- searchTwitter('vercel', n = 10000)%>%
twListToDF()
write.csv(tweets_vercel, paste0("/Users/hui/Dropbox/Hui/Netlify/Twitter/Data/tweets_vercel",
Sys.Date(), ".csv"),row.names = F)
tweets_zeit <- searchTwitter('zeit', n = 10000)%>%
twListToDF()
write.csv(tweets_zeit, paste0("/Users/hui/Dropbox/Hui/Netlify/Twitter/Data/tweets_zeit",
Sys.Date(), ".csv"),row.names = F)
| Topic | Duration | Materials |
| :--- | :---: | :---: |
| Introduction to Data Science | 45 mins | [Slides](https://course2020.scientistcafe.com/slides/01introduction/introduction#(1))   |
| Cloud Platform and Big Data | 45 mins | [Slides](https://docs.google.com/presentation/d/1jWO1-Nloms0x1btYLevUqTBcycZo1SV_cvE9rmg24tM/edit?usp=sharing)   |
| Break | 15 mins |  |
| Hands-on | 55 mins | [Databricks Community Edition](https://databricks.com/try-databricks), [R](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3981279215211072/4269117185296595/78755435857845/latest.html), [Load Spark Dataframe](https://github.com/happyrabbit/IntroDataScience/blob/master/Python/LoadDatasetSpark.ipynb), [Python](https://github.com/happyrabbit/IntroDataScience/blob/master/Python/PysparkETL.ipynb)  |
| Decision Tree | 45 mins |[Slides](https://docs.google.com/presentation/d/1Qx5lycRASX8yFRhTNgC-_a_9wPugra8Iji5vhd-r7mU/edit#slide=id.p7) |
| Hands-on| 15 mins | [R](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3981279215211072/2153523447602836/78755435857845/latest.html), Python  |
| Q&A | 10 mins |  |
|  |  |  |
| Feedforward Neural Network (FFNN) | 45 mins | [Slides](https://course2020.scientistcafe.com/slides/02DeepLearning/DNN/DNN_Intro.html)   |
| FFNN Hands-on | 60 mins | [R](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3981279215211072/4269117185296628/78755435857845/latest.html), [Python](https://github.com/happyrabbit/IntroDataScience/blob/master/Python/FFNN.ipynb)  |
| Break | 15 mins |  |
| Convolutional Neural Network (CNN)  | 30 mins | [Slides](https://course2020.scientistcafe.com/slides/02DeepLearning/CNN/CNN_Intro.html)   |
| CNN Hands-on | 20 mins | [R](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3981279215211072/1821707097719970/78755435857845/latest.html), [Python](https://github.com/happyrabbit/IntroDataScience/blob/master/Python/CNN.ipynb)  |
| Recurrent Neural Network (RNN) | 30 mins | [RNN Introduction](https://course2020.scientistcafe.com/slides/02DeepLearning/RNN/RNN_Intro.html), [Embedding](https://course2020.scientistcafe.com/slides/02DeepLearning/RNN/RNN_Embedding.html) |
| RNN Hands-on | 20 mins | [R](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3981279215211072/3137931017799475/78755435857845/latest.html), [Python](https://github.com/happyrabbit/IntroDataScience/blob/master/Python/RNN.ipynb), [Tokenize and Pad](https://github.com/happyrabbit/IntroDataScience/blob/master/Python/TokenizingPadding.ipynb) |
| Q&A | 10 mins |  |
150000/12
157 - 135
22000/12
rnorm(10)
exp(a2)/sum(exp(a2))
a2 = rnorm(10)
exp(a2)/sum(exp(a2))
round(exp(a2)/sum(exp(a2)),2)
a2 = rchisq(10, 9)
round(exp(a2)/sum(exp(a2)),2)
a2 = rchisq(10, 4)
round(exp(a2)/sum(exp(a2)),2)
a2 = rchisq(10, 1)
round(exp(a2)/sum(exp(a2)),2)
a2 = rchisq(10, 20)
round(exp(a2)/sum(exp(a2)),2)
a2 = rnorm(10, 0,6)
round(exp(a2)/sum(exp(a2)),2)
a2 = rnorm(10, 0,4)
round(exp(a2)/sum(exp(a2)),2)
a2 = rnorm(10, 0,2)
round(exp(a2)/sum(exp(a2)),2)
round(exp(a2)/sum(exp(a2)),2) %>%>sum()
sum(round(exp(a2)/sum(exp(a2)),2) )
round(exp(a2)/sum(exp(a2)),2)
399/12
1+1
rabbit="horse"
28*28
0.24954472^2
60000/64
60000/64*10
x2 = 38/100000
x1 = 0.2
10/800
p_needed <- c( "dplyr", "stringr","readr","dygraphs","ggplot2","plotly",
"rlang","lubridate","tidyr","DT","reshape2","DBI","RPostgreSQL",
"flexdashboard","SparkR","knitr","prettydoc","scales","knitr",
"cleanNLP","wordcloud","tm","tidytext","rmdformats","rmarkdown")
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
if (length(p_to_install) > 0) {
install.packages(p_to_install)
}
p_needed <- c( "dplyr", "stringr","readr","dygraphs","ggplot2","plotly",
"rlang","lubridate","tidyr","DT","reshape2","DBI","RPostgreSQL",
"flexdashboard","SparkR","knitr","prettydoc","scales","knitr",
"cleanNLP","wordcloud","tm","tidytext","rmdformats","rmarkdown","rethinking","caret")
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
if (length(p_to_install) > 0) {
install.packages(p_to_install)
}
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce
install.packages("rethinking")
devtools::install_github("rmcelreath/rethinking")
install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
devtools::install_github("rmcelreath/rethinking")
devtools::install_github("rmcelreath/rethinking")
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce
View(d)
devtools::install_github("hadley/emo")
knitr::opts_chunk$set(echo = FALSE)
library(dagitty)
View(d)
names(d)
dag <- dagitty( "dag {
MedianAgeMarriage -> Divorce
MedianAgeMarriage -> Marriage
Marriage -> Divorce
South -> WaffleHouses
South -> MedianAgeMarriage
}")
# coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag)
library(dagitty)
dagitty::drawdag( dag)
plot(graphLayout(dag))
library(dplyr)
d %>% select(WaffleHouses, MedianAgeMarriage, Marriage, Divorce) %>% head()
library(rethinking)
knitr::opts_chunk$set(echo = FALSE, message = F, error = F, warning =  F)
drawdag(dag)
?graphLayout
?drawdag
dag <- dagitty( "dag {
South -> WaffleHouses
South -> MedianAgeMarriage
MedianAgeMarriage -> Divorce
MedianAgeMarriage -> Marriage
Marriage -> Divorce
}")
drawdag(dag)
correlation = cor(d)
d = d %>% select(WaffleHouses, South, MedianAgeMarriage, Marriage, Divorce)
head(d)
correlation = cor(d)
corrplot.mixed(correlation, order = "hclust", tl.pos = "lt",
upper = "ellipse")
library(corrplot)
install.packages("corrplot")
library(corrplot)
correlation = cor(d)
corrplot.mixed(correlation, order = "hclust", tl.pos = "lt",
upper = "ellipse")
3386/14*30
library(dplyr)
library(rethinking)
data(WaffleDivorce)
d <- WaffleDivorce
names(d)
head(d)
750*4
5151/6 + 1624
getwd()
binom.test(2,8)
binom.test(2,8,0)
1789/1515
